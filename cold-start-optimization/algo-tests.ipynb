{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################\n",
    "#   Title: algo-tests.ipynb\n",
    "#   Authors: Nadia Goralski, Stacey Koornneef\n",
    "#   Code used to test accuracy of different KNN algorithms in Surprise\n",
    "#   \n",
    "#   References: \n",
    "#   https://surprise.readthedocs.io/en/stable/getting_started.html\n",
    "#######################################################################################\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from surprise import Dataset, KNNBaseline, KNNBasic, KNNWithMeans, KNNWithZScore, Reader\n",
    "from tabulate import tabulate\n",
    "\n",
    "\n",
    "# read user data\n",
    "survey = pd.read_csv('./survey_result.csv')\n",
    "df = pd.read_csv('./random.csv')\n",
    "# rating scale for case difficulty\n",
    "reader = Reader(rating_scale=(0, 4))\n",
    "\n",
    "# load data from dataframe\n",
    "data = Dataset.load_from_df(df[[\"survey_result_id\", \"errors\", \"adjust_level\"]], reader) \n",
    "\n",
    "options_dict = {\"name\": \"cosine\", \"user_based\": False}\n",
    "\n",
    "# different algorithms to test\n",
    "knn_baseline = KNNBaseline(sim_options=options_dict)\n",
    "knn_basic = KNNBasic(sim_options=options_dict)\n",
    "knn_means = KNNWithMeans(sim_options=options_dict)\n",
    "knn_zscore = KNNWithZScore(sim_options=options_dict)\n",
    "\n",
    "# put algorithms in a dictionary with their name\n",
    "algs = [ \n",
    "    {\"name\": \"KNNBaseline\", \"obj\": knn_baseline},\n",
    "    {\"name\": \"KNNBasic\", \"obj\": knn_basic},\n",
    "    {\"name\": \"KNNWithMeans\", \"obj\": knn_means},\n",
    "    {\"name\": \"KNNWithZScore\", \"obj\": knn_zscore}\n",
    "]\n",
    "\n",
    "# error counts to test\n",
    "NO_ERROR = 0\n",
    "LOW_ERROR = 2\n",
    "HIGH_ERROR = 5\n",
    "\n",
    "# adjust level function for manual adjusting\n",
    "def adjust_level(level, error_count):\n",
    "    if error_count > 2 and level > 0:\n",
    "        return level - 1\n",
    "    return level\n",
    "\n",
    "\n",
    "predictions = []\n",
    "trainset = data.build_full_trainset()\n",
    "\n",
    "# iterate through each survey result\n",
    "for index, row in survey.iterrows():\n",
    "\n",
    "    # iterate through each alg\n",
    "    for i, alg in enumerate(algs):\n",
    "        id = row['id']\n",
    "        knn = alg['obj']\n",
    "        knn.fit(trainset)\n",
    "\n",
    "        prediction_none = knn.predict(id, NO_ERROR)\n",
    "        prediction_low = knn.predict(id, LOW_ERROR)\n",
    "        prediction_high = knn.predict(id, HIGH_ERROR)\n",
    "        \n",
    "        #print(row)\n",
    "        predict =[id,\n",
    "                  alg['name'],\n",
    "                  row['level'],\n",
    "                  prediction_none.est,\n",
    "                  prediction_low.est,\n",
    "                  prediction_high.est,\n",
    "                  ]\n",
    "        # print(predict)          \n",
    "        predictions.append(predict)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ##################################\n",
    "# Comparison Tables \n",
    "# per Algorithm & Prediction Type\n",
    "# ##################################\n",
    "\n",
    "prediction_types = [[0, 'Prediction (No Errors)'], [2, 'Prediction (Low Errors)'], [5, 'Prediction (High Errors)']]\n",
    "table_header = ['Survey ID',  'Start Level']\n",
    "\n",
    "\n",
    "for a, alg in enumerate(algs):\n",
    "    name = alg['name']\n",
    "   \n",
    "    \n",
    "    for pt, ptype in enumerate(prediction_types):\n",
    "        error_count = ptype[0]\n",
    "        typename = ptype[1]\n",
    "\n",
    "        manual_vals = []\n",
    "        predict_vals = []\n",
    "    \n",
    "        \n",
    "        print(name, '-', typename, '\\n==================================================================')\n",
    "        th = table_header\n",
    "        th.append('Manual Suggested')\n",
    "        th.append('Predicted')\n",
    "        \n",
    "\n",
    "        table = [table_header]\n",
    "        algo_predictions = [p for p in predictions if p[1] == name]\n",
    "        #print(algo_predictions)\n",
    "        for algp in algo_predictions:\n",
    "            level = algp[2]\n",
    "            manual_adjust_level = adjust_level(level, error_count)\n",
    "            manual_vals.append(manual_adjust_level)\n",
    "            predict_val = algp[pt+3]\n",
    "            predict_vals.append(predict_val)\n",
    "            table.append([\n",
    "                algp[0],\n",
    "                algp[2],\n",
    "                manual_adjust_level,\n",
    "                predict_val,\n",
    "            ])\n",
    "        tab = tabulate(table, headers='firstrow', tablefmt='grid')\n",
    "        print(tab)\n",
    "        # predict_vals_round = np.round(np.array(predict_vals)) \n",
    "        # accuracy = balanced_accuracy_score(np.array(manual_vals), predict_vals_round)\n",
    "        # if (accuracy <= 0.5):\n",
    "        #     print(np.array(manual_vals))\n",
    "        #     print(predict_vals_round)\n",
    "        # print(\"Accuracy:\",accuracy)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ##################################\n",
    "# Accuracy Table\n",
    "# ##################################\n",
    "prediction_types = [[0, 'Prediction (No Errors)'], [2, 'Prediction (Low Errors)'], [5, 'Prediction (High Errors)']]\n",
    "table_header = ['Algo', 'Prediction (No Errors) Acc',  'Prediction (Low Errors) Acc', 'Prediction (High Errors) Acc']\n",
    "\n",
    "table = [table_header]\n",
    "for a, alg in enumerate(algs):\n",
    "    name = alg['name']\n",
    " \n",
    "    accuracies = []\n",
    "    for pt, ptype in enumerate(prediction_types):\n",
    "        error_count = ptype[0]\n",
    "        typename = ptype[1]\n",
    "\n",
    "        manual_vals = []\n",
    "        predict_vals = []\n",
    "    \n",
    "        \n",
    "        # print(name, '-', typename, '\\n==================================================================')\n",
    "        th = table_header\n",
    "        th.append(ptype)\n",
    "    \n",
    "\n",
    "        \n",
    "        algo_predictions = [p for p in predictions if p[1] == name]\n",
    "        #print(algo_predictions)\n",
    "        for algp in algo_predictions:\n",
    "            level = algp[2]\n",
    "            manual_adjust_level = adjust_level(level, error_count)\n",
    "            manual_vals.append(manual_adjust_level)\n",
    "            predict_val = algp[pt+3]\n",
    "            predict_vals.append(predict_val)\n",
    "\n",
    "        # round to get closest level\n",
    "        predict_vals_round = np.round(np.array(predict_vals)) \n",
    "        accuracy = balanced_accuracy_score(np.array(manual_vals), predict_vals_round)\n",
    "        accuracies.append(accuracy)\n",
    "        # print(\"Accuracy:\",accuracy)\n",
    "    table.append([\n",
    "        name, \n",
    "        accuracies[0],\n",
    "        accuracies[1],\n",
    "        accuracies[2],\n",
    "    ])\n",
    "\n",
    "tab = tabulate(table, headers='firstrow')\n",
    "print(tab)\n",
    "    \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e534e48711db4d1e1c48977d0d14ff85b1f16d41bcc4fdfd88268a329b3c9d66"
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}